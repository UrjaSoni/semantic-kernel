{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c93ac5b",
   "metadata": {},
   "source": [
    "# Running Native Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {},
   "source": [
    "Two of the previous notebooks showed how to [execute semantic functions inline](./03-semantic-function-inline.ipynb) and how to [run prompts from a file](./02-running-prompts-from-file.ipynb).\n",
    "\n",
    "In this notebook, we'll show how to use native functions from a file. We will also show how to call semantic functions from native functions.\n",
    "\n",
    "This can be useful in a few scenarios:\n",
    "\n",
    "- Writing logic around how to run a prompt that changes the prompt's outcome.\n",
    "- Using external data sources to gather data to concatenate into your prompt.\n",
    "- Validating user input data prior to sending it to the LLM prompt.\n",
    "\n",
    "Native functions are defined using standard Python code. The structure is simple, but not well documented at this point.\n",
    "\n",
    "The following examples are intended to help guide new users towards successful native & semantic function use with the SK Python framework.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d90b0c13",
   "metadata": {},
   "source": [
    "Prepare a semantic kernel instance first, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da651d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel==0.9.2b1 in /opt/homebrew/lib/python3.10/site-packages (0.9.2b1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (3.9.3)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (0.7.1)\n",
      "Requirement already satisfied: grpcio>=1.50.0 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (1.62.1)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.2 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.25 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (1.12.0)\n",
      "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (0.18.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (23.6.21.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (2.6.3)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (1.0.1)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (2023.12.25)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.10/site-packages (from semantic-kernel==0.9.2b1) (1.10.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.2b1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.2b1) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.2b1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.2b1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.2b1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.2b1) (4.0.3)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from motor<4.0.0,>=3.3.2->semantic-kernel==0.9.2b1) (4.6.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.2b1) (3.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.2b1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.2b1) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.2b1) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.2b1) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.2b1) (4.9.0)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (3.7.2)\n",
      "Requirement already satisfied: isodate in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.7.1)\n",
      "Requirement already satisfied: parse in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in /opt/homebrew/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (3.0.1)\n",
      "Requirement already satisfied: chardet>=3.0 in /opt/homebrew/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.2b1) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in /opt/homebrew/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.2b1) (0.17.17)\n",
      "Requirement already satisfied: requests>=2.25 in /opt/homebrew/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.2b1) (2.31.0)\n",
      "Requirement already satisfied: six~=1.15 in /opt/homebrew/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.2b1) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/homebrew/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.2b1) (23.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=2->semantic-kernel==0.9.2b1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=2->semantic-kernel==0.9.2b1) (2.16.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==0.9.2b1) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.2b1) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.2b1) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.2b1) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.18.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (6.0)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/homebrew/lib/python3.10/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /opt/homebrew/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (0.3.2)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /opt/homebrew/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/homebrew/lib/python3.10/site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.2->semantic-kernel==0.9.2b1) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.2b1) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.2b1) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.10/site-packages (from werkzeug->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.2b1) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install semantic-kernel==0.9.2b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fddb5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import Service\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = Service.AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd150646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "if selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    service_id = \"aoai_chat\"  # used later in the notebook\n",
    "    azure_chat_service = AzureChatCompletion(\n",
    "        service_id=service_id, deployment_name=deployment, endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your chat model\n",
    "    kernel.add_service(azure_chat_service)\n",
    "\n",
    "# Configure OpenAI service\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    service_id = \"oai_chat\"  # used later in the notebook\n",
    "    oai_chat_service = OpenAIChatCompletion(\n",
    "        service_id=service_id, ai_model_id=\"gpt-4-turbo-1106\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "    kernel.add_service(oai_chat_service)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "186767f8",
   "metadata": {},
   "source": [
    "Let's create a **native** function that gives us a random number between 3 and a user input as the upper limit. We'll use this number to create 3-x paragraphs of text when passed to a semantic function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "First, let's create our native function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae29c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 3-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Generate a random number between 3-x\",\n",
    "        name=\"GenerateNumberThreeOrHigher\",\n",
    "    )\n",
    "    def generate_number_three_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 3-<input>\n",
    "        Example:\n",
    "            \"8\" => rand(3,8)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(3, int(input)))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26b90c4",
   "metadata": {},
   "source": [
    "Next, let's create a semantic function that accepts a number as `{{$input}}` and generates that number of paragraphs about two Corgis on an adventure. `$input` is a default variable semantic functions can use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7890943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$input}} paragraphs long. It must be this length.\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo-1106\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=deployment,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"story\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.create_function_from_prompt(\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "generate_number_plugin = kernel.import_plugin_from_object(GenerateNumberPlugin(), \"GenerateNumberPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2471c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_plugin[\"GenerateNumberThreeOrHigher\"]\n",
    "number_result = await generate_number_three_or_higher(kernel, input=6)\n",
    "print(number_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f043a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = await corgi_story.invoke(kernel, input=number_result.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245e7a2",
   "metadata": {},
   "source": [
    "_Note: depending on which model you're using, it may not respond with the proper number of paragraphs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a60e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 5 paragraphs long.\n",
      "=====================================================\n",
      "Once upon a time in a small town called Pawsomeville, there lived two adventurous Corgis named Charlie and Daisy. They were the best of friends, always seeking new and exciting experiences. One sunny morning, they decided to embark on a thrilling adventure to explore the nearby enchanted forest.\n",
      "\n",
      "As they entered the forest, Charlie and Daisy were greeted by the sweet melody of chirping birds and the rustling of leaves beneath their paws. They marveled at the tall trees and colorful flowers that surrounded them. Excitement filled their hearts as they ventured deeper into the unknown.\n",
      "\n",
      "In the heart of the forest, they stumbled upon a sparkling river. Its crystal-clear water danced under the golden sunlight, inviting them to take a refreshing dip. Charlie and Daisy eagerly jumped in, splashing and playing together. They were reminded of the importance of having fun and cherishing each moment.\n",
      "\n",
      "After their playful swim, they stumbled upon a group of woodland creatures. There were rabbits, squirrels, and even a wise old owl named Oliver. The animals were initially cautious, but Charlie and Daisy's friendly nature soon won them over. They spent hours sharing stories and laughter, discovering that friendship knows no boundaries.\n",
      "\n",
      "As the sun began to set, painting the sky with vibrant hues, Charlie and Daisy knew it was time to head back home. They bid farewell to their newfound friends, carrying with them memories that would last a lifetime. They realized that every adventure, no matter how big or small, taught them valuable lessons about friendship, kindness, and the beauty of nature.\n",
      "\n",
      "When they returned to Pawsomeville, Charlie and Daisy couldn't contain their excitement. They shared their adventure with their families and friends, inspiring others to explore the wonders of the world around them. From that day forward, the two Corgis became ambassadors of joy, reminding everyone to embrace every moment and cherish the magic of friendship.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story exactly {number_result.value} paragraphs long.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef29d16",
   "metadata": {},
   "source": [
    "## Kernel Functions with Annotated Parameters\n",
    "\n",
    "That works! But let's expand on our example to make it more generic.\n",
    "\n",
    "For the native function, we'll introduce the lower limit variable. This means that a user will input two numbers and the number generator function will pick a number between the first and second input.\n",
    "\n",
    "We'll make use of the Python's `Annotated` class to hold these variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54983d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    OpenAIChatCompletion,\n",
    ")\n",
    "\n",
    "if sys.version_info >= (3, 9):\n",
    "    from typing import Annotated\n",
    "else:\n",
    "    from typing_extensions import Annotated\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "if selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    service_id = \"aoai_chat\"  # used later in the notebook\n",
    "    azure_chat_service = AzureChatCompletion(\n",
    "        service_id=service_id, deployment_name=deployment, endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your chat model\n",
    "    kernel.add_service(azure_chat_service)\n",
    "\n",
    "# Configure OpenAI service\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    service_id = \"oai_chat\"  # used later in the notebook\n",
    "    oai_chat_service = OpenAIChatCompletion(\n",
    "        service_id=service_id, ai_model_id=\"gpt-4-turbo-1106\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "    kernel.add_service(oai_chat_service)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "091f45e4",
   "metadata": {},
   "source": [
    "Let's start with the native function. Notice that we're add the `@kernel_function` decorator that holds the name of the function as well as an optional description. The input parameters are configured as part of the function's signature, and we use the `Annotated` type to specify the required input arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between a min and a max.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"GenerateNumber\",\n",
    "        description=\"Generate a random number between min and max\",\n",
    "    )\n",
    "    def generate_number(\n",
    "        self,\n",
    "        min: Annotated[int, \"the minimum number of paragraphs\"],\n",
    "        max: Annotated[int, \"the maximum number of paragraphs\"] = 10,\n",
    "    ) -> Annotated[int, \"the output is a number\"]:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min=\"4\" max=\"10\" => rand(4,8)\n",
    "        Args:\n",
    "            min -- The lower limit for the random number generation\n",
    "            max -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(min, max))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {min} and {max}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bcdf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_number_plugin = kernel.import_plugin_from_object(GenerateNumberPlugin(), \"GenerateNumberPlugin\")\n",
    "generate_number = generate_number_plugin[\"GenerateNumber\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ad068d6",
   "metadata": {},
   "source": [
    "Now let's also allow the semantic function to take in additional arguments. In this case, we're going to allow the our CorgiStory function to be written in a specified language. We'll need to provide a `paragraph_count` and a `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b8286fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo-1106\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=deployment,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"summarize\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.create_function_from_prompt(\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8778bad",
   "metadata": {},
   "source": [
    "Let's generate a paragraph count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28820d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 1 paragraphs long.\n"
     ]
    }
   ],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value\n",
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a9147",
   "metadata": {},
   "source": [
    "We can now invoke our corgi_story function using the `kernel` and the keyword arguments `paragraph_count` and `language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe07c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output to the semantic story function\n",
    "desired_language = \"Spanish\"\n",
    "story = await corgi_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6732a30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 1 paragraphs long in Spanish.\n",
      "=====================================================\n",
      "Había una vez, en un pequeño pueblo, dos adorables corgis llamados Coco y Max. Eran los mejores amigos y siempre estaban juntos, pero un día sintieron la necesidad de aventurarse más allá de su hogar. Decidieron explorar el hermoso bosque que se extendía más allá de las colinas verdes.\n",
      "\n",
      "Excitados, Coco y Max se adentraron en el bosque, saltando y corriendo entre los árboles. Descubrieron hermosas flores y mariposas de colores brillantes que revoloteaban a su alrededor. Pero mientras continuaban su viaje, se encontraron con un pequeño riachuelo que les bloqueaba el camino.\n",
      "\n",
      "Sin embargo, en lugar de rendirse, Coco y Max trabajaron juntos para encontrar una solución. Max saltó de roca en roca, mientras Coco lo esperaba pacientemente al otro lado. Finalmente, llegaron al otro lado, pero se encontraron con un acantilado muy alto.\n",
      "\n",
      "Coco y Max no se rindieron y buscaron una manera de superar el obstáculo. Coco tuvo una brillante idea y encontró una rama larga y resistente. Max, con su pequeño tamaño, se subió en la rama y Coco lo levantó cuidadosamente hasta el otro lado. Una vez más, trabajaron juntos y superaron el desafío.\n",
      "\n",
      "Después de muchas aventuras y obstáculos, Coco y Max finalmente llegaron a un hermoso prado lleno de flores y pájaros cantando. Se dieron cuenta de que no importa cuán grandes o pequeños sean, siempre pueden superar cualquier desafío si trabajan juntos y no se rinden.\n",
      "\n",
      "Con sus colas agitándose de alegría, Coco y Max disfrutaron de su merecido descanso en el prado, agradecidos por su amistad y por todas las maravillosas experiencias que habían vivido juntos. Desde ese día, siempre recordaron que la amistad y el trabajo en equipo son la clave para superar cualquier obstáculo que se les presente.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb786c54",
   "metadata": {},
   "source": [
    "## Calling Native Functions within a Semantic Function\n",
    "\n",
    "One neat thing about the Semantic Kernel is that you can also call native functions from within Prompt Functions!\n",
    "\n",
    "We will make our CorgiStory semantic function call a native function `GenerateNames` which will return names for our Corgi characters.\n",
    "\n",
    "We do this using the syntax `{{plugin_name.function_name}}`. You can read more about our prompte templating syntax [here](../../../docs/PROMPT_TEMPLATE_LANGUAGE.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d84c7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNamesPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate character names.\n",
    "    \"\"\"\n",
    "\n",
    "    # The default function name will be the name of the function itself, however you can override this\n",
    "    # by setting the name=<name override> in the @kernel_function decorator. In this case, we're using\n",
    "    # the same name as the function name for simplicity.\n",
    "    @kernel_function(description=\"Generate character names\", name=\"generate_names\")\n",
    "    def generate_names(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate two names.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        names = {\"Hoagie\", \"Hamilton\", \"Bacon\", \"Pizza\", \"Boots\", \"Shorts\", \"Tuna\"}\n",
    "        first_name = random.choice(list(names))\n",
    "        names.remove(first_name)\n",
    "        second_name = random.choice(list(names))\n",
    "        return f\"{first_name}, {second_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ab7d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_names_plugin = kernel.import_plugin_from_object(GenerateNamesPlugin(), plugin_name=\"GenerateNames\")\n",
    "generate_names = generate_names_plugin[\"generate_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94decd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "- The two names of the corgis are {{GenerateNames.generate_names}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be72a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo-1106\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=deployment,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"corgi-new\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.create_function_from_prompt(\n",
    "    function_name=\"CorgiStoryUpdated\",\n",
    "    plugin_name=\"CorgiPluginUpdated\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e6cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e980348",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_language = \"French\"\n",
    "story = await corgi_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4ade048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 5 paragraphs long in French.\n",
      "=====================================================\n",
      "Il était une fois, dans un petit village pittoresque situé au cœur de la campagne, deux adorables corgis nommés Boots et Shorts. Ces deux amis inséparables étaient connus pour leur énergie débordante et leur curiosité sans limites. Un jour, ils décidèrent de partir à l'aventure, à la recherche de nouvelles découvertes et de rencontres excitantes.\n",
      "\n",
      "Les deux corgis marchaient joyeusement le long d'un sentier bordé de fleurs colorées. Ils se sentaient libres et enthousiastes à l'idée de ce voyage improvisé. Leurs petites pattes agiles les emmenèrent vers une forêt mystérieuse, où une légende racontait l'existence d'une cascade magique.\n",
      "\n",
      "Boots et Shorts s'aventurèrent plus profondément dans la forêt, guidés par le bruit apaisant de l'eau qui tombait. Soudain, ils aperçurent une magnifique cascade scintillante, illuminée par les rayons du soleil. Les corgis étaient émerveillés par cette vue merveilleuse et se précipitèrent vers la cascade, éclaboussant joyeusement dans l'eau fraîche.\n",
      "\n",
      "Alors qu'ils jouaient, leurs yeux furent attirés par un petit oiseau exotique qui se reposait sur une branche voisine. Intrigués, ils s'approchèrent doucement, évitant de l'effrayer. L'oiseau, nommé Coco, était amical et bavard. Il leur raconta des histoires incroyables sur les terres lointaines qu'il avait explorées.\n",
      "\n",
      "Les corgis écoutèrent attentivement Coco, inspirés par son récit. Ils réalisèrent que l'aventure n'était pas seulement une question de voyages physiques, mais aussi de découvertes intérieures et de rencontres inattendues. Boots et Shorts comprirent que chaque pas en avant dans la vie pouvait être une aventure en soi, remplie d'opportunités et de joie.\n",
      "\n",
      "Après cette rencontre magique, les corgis rentrèrent chez eux, le cœur rempli de gratitude et d'excitation pour de nouvelles aventures à venir. Ils savaient maintenant que chaque jour pouvait être une occasion de découvrir quelque chose de nouveau et de s'émerveiller de la beauté du monde qui les entourait. Avec leurs petites queues qui remuaient de bonheur, Boots et Shorts savaient qu'ils étaient prêts pour de nouvelles aventures extraordinaires dans leur vie de corgis curieux et aimants.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42f0c472",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "A quick review of what we've learned here:\n",
    "\n",
    "- We've learned how to create native and prompt functions and register them to the kernel\n",
    "- We've seen how we can use Kernel Arguments to pass in more custom variables into our prompt\n",
    "- We've seen how we can call native functions within a prompt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
